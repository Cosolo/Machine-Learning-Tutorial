{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision trees** are one of the most popular algorithms used in machine learning, mostly for ***classification*** but also for ***regression*** problems. Our brain works like a decision tree every time we ask ourselves a question before making a decision. For example: is it cloudy outside? If yes, I will bring an umbrella.\n",
    "\n",
    "- Decision trees where the target variable can take a discrete set of values are called classification trees. \n",
    "- Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. \n",
    "\n",
    "When training a dataset to classify a variable, the idea of the Decision Tree is to divide the data into smaller datasets based on a certain feature value until the target variables all fall under one category. While the human brain decides to pick the “splitting feature” based on the experience (i.e. the cloudy sky), a computer splits the dataset based on the maximum ***information gain***. \n",
    "\n",
    "***Information gain***\n",
    "Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the purest daughter nodes. A commonly used measure of purity is called information. For each node of the tree, the information value measures how much information a feature gives us about the class. The split with the highest information gain will be taken as the first split and the process will continue until all children nodes are pure, or until the information gain is 0.\n",
    "\n",
    "![](img/DecisionTree/InformationGain.png)\n",
    "A decision tree is drawn upside down with its root at the top (***root node***). In the image above, the middle step represents a ***condition/internal node***, based on which the tree splits into ***branches/ edges***. The end of the branch that doesn’t split anymore is the ***decision/leaf***, in this case, which animal is it.\n",
    "\n",
    "**Entropy**\n",
    "Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.\n",
    "\n",
    "$$ \\large \\sum_{i=1}^{k} P(value_i) \\log_2 (P(value_i)) $$\n",
    "\n",
    "Entropy calculation on an example, dataset contains: 3 giraffes, 2 tigers, 1 monkey, 2 elephants = 8 animals in total.\n",
    "\n",
    "$$ ENTROPY = (\\frac{3}{8}) \\log_2 (\\frac{3}{8}) + (\\frac{2}{8}) \\log_2 (\\frac{2}{8}) + (\\frac{1}{8}) \\log_2 (\\frac{1}{8}) + (\\frac{2}{8}) \\log_2 (\\frac{2}{8}) $$<br>\n",
    "$$ ENTROPY = 0.571 $$\n",
    "\n",
    "Entropy is measured between 0 and 1.(Depending on the number of classes in the dataset, entropy can be greater than 1 but it means the same thing , a very high level of disorder).\n",
    "\n",
    "\n",
    "**Steps for Making decision tree:**\n",
    "-  Get list of rows (dataset) which are taken into consideration for making decision tree (recursively at each nodes).\n",
    "-  Calculate uncertanity of our dataset or Gini impurity or how much our data is mixed up etc.\n",
    "-  Generate list of all question which needs to be asked at that node.\n",
    "-  Partition rows into True rows and False rows based on each question asked.\n",
    "-  Calculate information gain based on gini impurity and partition of data from previous step.\n",
    "-  Update highest information gain based on each question asked.\n",
    "-  Update best question based on information gain (higher information gain).\n",
    "-  Divide the node on best question. Repeat again from step 1 again until we get pure node (leaf nodes).\n",
    "\n",
    "**Advantages of Decision Tree:**\n",
    "\n",
    "-  Easy to use and understand.\n",
    "-  Can handle both categorical and numerical data.\n",
    "-  Resistant to outliers, hence require little data preprocessing.\n",
    "\n",
    "**Disadvantages of Decision Tree:**\n",
    "\n",
    "-  Prone to overfitting.\n",
    "-  Require some kind of measurement as to how well they are doing.\n",
    "-  Need to be careful with parameter tuning.\n",
    "-  Can create biased learned trees if some classes dominate.\n",
    "\n",
    "**Great sources:**\n",
    "-  https://towardsdatascience.com/decision-tree-an-algorithm-that-works-like-the-human-brain-8bc0652f1fc6\n",
    "-  https://towardsdatascience.com/decision-tree-in-machine-learning-e380942a4c96\n",
    "-  https://www.youtube.com/watch?v=RmajweUFKvM&list=PLEiEAq2VkUULYYgj13YHUWmRePqiu8Ddy&index=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Balance Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_data = pd.read_csv('data/balance-scale.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lenght::  624\n",
      "Dataset Shape::  (624, 5)\n"
     ]
    }
   ],
   "source": [
    "print (\"Dataset Lenght:: \", len(balance_data))\n",
    "print (\"Dataset Shape:: \", balance_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:: \n",
      "   B  1  1.1  1.2  1.3\n",
      "0  R  1    1    1    2\n",
      "1  R  1    1    1    3\n",
      "2  R  1    1    1    4\n",
      "3  R  1    1    1    5\n",
      "4  R  1    1    2    1\n"
     ]
    }
   ],
   "source": [
    "print (\"Dataset:: \")\n",
    "print(balance_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the target variables\n",
    "X = balance_data.values[:, 1:5]\n",
    "Y = balance_data.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=5, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# Function to perform training with Entropy\n",
    "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=3, min_samples_leaf=5)\n",
    "print(clf_entropy.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'L' 'L'\n",
      " 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R'\n",
      " 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'L'\n",
      " 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'L' 'R' 'L'\n",
      " 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R'\n",
      " 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L'\n",
      " 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R'\n",
      " 'R' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L'\n",
      " 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R']\n"
     ]
    }
   ],
   "source": [
    "# Function to make Prediction\n",
    "y_pred_en = clf_entropy.predict(X_test)\n",
    "print(y_pred_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  66.48936170212765\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy\n",
    "print (\"Accuracy is \", accuracy_score(y_test,y_pred_en)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
